# base
name: sdac

# for RL
norm_obs: true
norm_reward: false
discount_factor: 0.99
n_steps: 1000
n_update_steps: 10000
critic_lr: 3e-4
n_critic_iters: 40
max_grad_norm: 1.0
gae_coeff: 0.97
len_replay_buffer: 100000
n_target_quantiles: 50

# for trust region
max_kl: 0.001
damping_coeff: 0.01
num_conjugate: 10
line_decay: 0.8

# for constraint
con_thresholds: [0.025]
con_alphas: [0.25]

# entropy
con_entropy: false
con_ent_thresh: -1.0

# for logging
logging:
    cost_indep: [fps, reward_sum, eplen, kl, entropy, safety_mode, objective, reward_critic_loss, cost_critic_loss]
    cost_dep: [cost_sum, constraints]

# for model
model:
    actor:
        mlp:
            shape: [512, 512]
            activation: LeakyReLU
        use_action_bound: true
        log_std_init: 0.0
        log_std_fix: false
    reward_critic:
        mlp:
            shape: [512, 512]
            activation: LeakyReLU
        clip_range: [-np.inf, np.inf]
        n_critics: 2
        n_quantiles: 25
    cost_critic:
        mlp:
            shape: [512, 512]
            activation: LeakyReLU
        last_activation: softplus
        clip_range: [0.0, 100.0]
        n_critics: 2
        n_quantiles: 25
